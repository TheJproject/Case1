train_ind <- sample(n, size = smp_size)
# define into test and train
n = dim(x)[1] # number of observations
smp_size = 80
set.seed(123456789)
train_ind <- sample(100, size = smp_size)
test <- (1:100)[-train_ind]
splitting = data.frame('train'=train_ind,'test'=test)
write.csv(splitting,"C:\\Users\\Nicolaj\\Documents\\GitHub\\Case1\\split.csv",row.names = F)
# import data:
data = read.csv("case1Data.txt",sep=",")
train.test = read.csv("split.csv",sep=",")
# data is investigated in seperate file
# the first col is y, the last 5 columns contains categorical features
# w missing values, NaN:
index.cat = (dim(data)[2]-4):(dim(data)[2])
# NA as an instance i.e. just factor the last columns
data.inst = data.frame(data)
data.inst[,index.cat] = lapply(data.inst[,index.cat], factor)
nan.class = as.character(data$C_1[1])
data[data==nan.class] <- NA
data[,index.cat] = lapply(data[,index.cat], factor)
# Impute the NAs using nonparametric method:
# only do it on the x subset:
imp =  missForest(data[,-1])$ximp
data.impute = cbind('y'=data$y,imp)
# one hot encode
encoder = onehot(data)
data.one.hot = as.data.frame(predict(encoder,data))
train.inst = data.inst[train.test$train,]
test.inst = data.inst[train.test$test,]
levels(test.inst[,index.cat]) = lapply(levels(train.inst[,index.cat]), levels)
test.inst
test.inst$C_2
test.inst$C_1
test.inst$C_4
test.inst$C_3
test.inst$C_5
# Impute the NAs using nonparametric method:
# only do it on the x subset:
imp =  missForest(data[,-1])$ximp
data.impute = cbind('y'=data$y,imp)
# one hot encode
encoder = onehot(data)
data.one.hot = as.data.frame(predict(encoder,data))
train.inst = data.inst[train.test$train,]
test.inst = data.inst[train.test$test,]
levels(test.inst[,index.cat]) = lapply(levels(train.inst[,index.cat]), levels)
train.impute = data.impute[train.test$train,]
test.inst = data.impute[train.test$test,]
train.one.hot = data.one.hot[train.test$train,]
test.one.hot = data.one.hot[train.test$test,]
#### define evaluation function #####
rmse_reg <- function(model_obj, testing = NULL, actual = NULL) {
#Calculates rmse for a regression decision tree
#Arguments:
# testing - test data set
# target  - target variable (length 1 character vector)
yhat <- predict(model_obj, newdata = testing)
sqrt(mean((yhat-actual)^2))
}
# we use 10 cross validation
set.seed(123)
regTree.inst=rpart(y~., data=train.inst, method="anova",control =rpart.control(minsplit =1,minbucket=1,xval=10))
printcp(regTree.inst)
# in the following we prune the tree using 1-SE
CVerr=regTree.inst$cptable[,"xerror"]
minCP=regTree.inst$cptable[which.min(regTree.inst$cptable[,"xerror"]),"CP"]
minSE=min(CVerr)+regTree.inst$cptable[which.min(CVerr),"xstd"]
w=which.min(CVerr[CVerr>minSE])
wv=which(CVerr>minSE)[w]
CP_SE=regTree.inst$cptable[wv,"CP"]
# prune the tress
pruned.tree.inst <- prune(regTree.inst, cp = CP_SE)
prp(pruned.tree.inst)
# on traning data
rmse_reg(regTree.inst,train.inst[,-1],train.inst[,1])
rmse_reg(pruned.tree.inst,train.inst[,-1],train.inst[,1])
# on test data
rmse_reg(regTree.inst,test.inst[,-1],test.inst[,1])
rmse_reg(pruned.tree.inst,test.inst[,-1],test.inst[,1])
# in other script number of trees 80, nodesize=5 test mtry:
bestmtry <- tuneRF(x=train.inst[,-1], y=train.inst[,1], stepFactor=1.5, improve=1e-10, ntree=80)
print(bestmtry)
# trains
rf.inst = randomForest(x = train.inst[,-1],y = train.inst[,1],ntree=80,mtry=80,nodesize=5)
# on traning data
rmse_reg(rf.inst,train.inst[,-1],train.inst[,1])
# on test data
rmse_reg(rf.inst,test.inst[,-1],test.inst[,1])
intersect(names(train),names(test))
# to fix bug of different levels in train and test
test.inst <- rbind(train.inst[1, ] , test.inst)
test.inst <- xtest[-1,]
test.inst <- test.inst[-1,]
# on test data
rmse_reg(rf.inst,test.inst[,-1],test.inst[,1])
# on traning data
rmse_reg(rf.inst,train.inst[,-1],train.inst[,1])
# to fix bug of different levels in train and test
test.inst <- rbind(train.inst[1, ] , test.inst)
test.inst <- test.inst[-1,]
# on test data
rmse_reg(rf.inst,test.inst[,-1],test.inst[,1])
# trains
rf.inst = randomForest(x = train.inst[,-1],y = train.inst[,1],ntree=500,mtry=80,nodesize=5)
# on traning data
rmse_reg(rf.inst,train.inst[,-1],train.inst[,1])
# to fix bug of different levels in train and test
test.inst <- rbind(train.inst[1, ] , test.inst)
test.inst <- test.inst[-1,]
# on test data
rmse_reg(rf.inst,test.inst[,-1],test.inst[,1])
# trains
rf.inst = randomForest(x = train.inst[,-1],y = train.inst[,1],ntree=80,mtry=80,nodesize=5)
# on traning data
rmse_reg(rf.inst,train.inst[,-1],train.inst[,1])
# to fix bug of different levels in train and test
test.inst <- rbind(train.inst[1, ] , test.inst)
test.inst <- test.inst[-1,]
# on test data
rmse_reg(rf.inst,test.inst[,-1],test.inst[,1])
# trains
rf.inst = randomForest(x = train.inst[,-1],y = train.inst[,1],ntree=80,mtry=20,nodesize=5)
# on traning data
rmse_reg(rf.inst,train.inst[,-1],train.inst[,1])
# to fix bug of different levels in train and test
test.inst <- rbind(train.inst[1, ] , test.inst)
test.inst <- test.inst[-1,]
# on test data
rmse_reg(rf.inst,test.inst[,-1],test.inst[,1])
# trains
rf.inst = randomForest(x = train.inst[,-1],y = train.inst[,1],ntree=80,mtry=40,nodesize=5)
# on traning data
rmse_reg(rf.inst,train.inst[,-1],train.inst[,1])
# to fix bug of different levels in train and test
test.inst <- rbind(train.inst[1, ] , test.inst)
test.inst <- test.inst[-1,]
# on test data
rmse_reg(rf.inst,test.inst[,-1],test.inst[,1])
# trains
rf.inst = randomForest(x = train.inst[,-1],y = train.inst[,1],ntree=80,mtry=40,nodesize=1)
# on traning data
rmse_reg(rf.inst,train.inst[,-1],train.inst[,1])
# to fix bug of different levels in train and test
test.inst <- rbind(train.inst[1, ] , test.inst)
test.inst <- test.inst[-1,]
# on test data
rmse_reg(rf.inst,test.inst[,-1],test.inst[,1])
gb.isnst = blackboost(y ~ .,data=train.inst)
gb.inst = blackboost(y ~ .,data=train.inst)
# on traning data
rmse_reg(gb.inst,train.inst[,-1],train.inst[,1])
# on test data
rmse_reg(gb.inst,test.inst[,-1],test.inst[,1])
# we use 10 cross validation
set.seed(123)
regTree.impute=rpart(y~., data=train.impute, method="anova",control =rpart.control(minsplit =1,minbucket=1,xval=10))
printcp(regTree.impute)
# in the following we prune the tree using 1-SE
CVerr=regTree.impute$cptable[,"xerror"]
minCP=regTree.impute$cptable[which.min(regTree.impute$cptable[,"xerror"]),"CP"]
minSE=min(CVerr)+regTree.impute$cptable[which.min(CVerr),"xstd"]
w=which.min(CVerr[CVerr>minSE])
wv=which(CVerr>minSE)[w]
CP_SE=regTree.impute$cptable[wv,"CP"]
# prune the tress
pruned.tree.impute <- prune(regTree.impute, cp = CP_SE)
prp(pruned.tree.impute)
# on traning data
rmse_reg(regTree.impute,train.impute[,-1],train.impute[,1])
rmse_reg(pruned.tree.impute,train.impute[,-1],train.impute[,1])
# on test data
rmse_reg(regTree.impute,test.impute[,-1],test.impute[,1])
rmse_reg(pruned.tree.impute,test.impute[,-1],test.impute[,1])
train.impute = data.impute[train.test$train,]
test.impute = data.impute[train.test$test,]
# we use 10 cross validation
set.seed(123)
regTree.impute=rpart(y~., data=train.impute, method="anova",control =rpart.control(minsplit =1,minbucket=1,xval=10))
printcp(regTree.impute)
# in the following we prune the tree using 1-SE
CVerr=regTree.impute$cptable[,"xerror"]
minCP=regTree.impute$cptable[which.min(regTree.impute$cptable[,"xerror"]),"CP"]
minSE=min(CVerr)+regTree.impute$cptable[which.min(CVerr),"xstd"]
w=which.min(CVerr[CVerr>minSE])
wv=which(CVerr>minSE)[w]
CP_SE=regTree.impute$cptable[wv,"CP"]
# prune the tress
pruned.tree.impute <- prune(regTree.impute, cp = CP_SE)
prp(pruned.tree.impute)
# on traning data
rmse_reg(regTree.impute,train.impute[,-1],train.impute[,1])
rmse_reg(pruned.tree.impute,train.impute[,-1],train.impute[,1])
# on test data
rmse_reg(regTree.impute,test.impute[,-1],test.impute[,1])
rmse_reg(pruned.tree.impute,test.impute[,-1],test.impute[,1])
# in other script number of trees 80, nodesize=5 test mtry:
bestmtry <- tuneRF(x=train.impute[,-1], y=train.impute[,1], stepFactor=1.5, improve=1e-10, ntree=80)
print(bestmtry)
# trains
rf.impute = randomForest(x = train.impute[,-1],y = train.impute[,1],ntree=100,mtry=40,nodesize=5)
# trains
rf.impute = randomForest(x = train.impute[,-1],y = train.impute[,1],ntree=80,mtry=100,nodesize=5)
# on traning data
rmse_reg(rf.impute,train.impute[,-1],train.impute[,1])
# to fix bug of different levels in train and test
test.impute <- rbind(train.impute[1, ] , test.impute)
test.impute <- test.impute[-1,]
# on test data
rmse_reg(rf.impute,test.impute[,-1],test.impute[,1])
# trains
rf.impute = randomForest(x = train.impute[,-1],y = train.impute[,1],ntree=500,mtry=100,nodesize=5)
# on traning data
rmse_reg(rf.impute,train.impute[,-1],train.impute[,1])
# to fix bug of different levels in train and test
test.impute <- rbind(train.impute[1, ] , test.impute)
test.impute <- test.impute[-1,]
# on test data
rmse_reg(rf.impute,test.impute[,-1],test.impute[,1])
gb.impute = blackboost(y ~ .,data=train.impute)
# on traning data
rmse_reg(gb.impute,train.impute[,-1],train.impute[,1])
# on test data
rmse_reg(gb.impute,test.impute[,-1],test.impute[,1])
# trains
rf.impute = randomForest(x = train.impute[,-1],y = train.impute[,1],ntree=500,mtry=110,nodesize=5)
# on traning data
rmse_reg(rf.impute,train.impute[,-1],train.impute[,1])
# to fix bug of different levels in train and test
test.impute <- rbind(train.impute[1, ] , test.impute)
test.impute <- test.impute[-1,]
# on test data
rmse_reg(rf.impute,test.impute[,-1],test.impute[,1])
gb.impute = blackboost(y ~ .,data=train.impute)
# on traning data
rmse_reg(gb.impute,train.impute[,-1],train.impute[,1])
# on test data
rmse_reg(gb.impute,test.impute[,-1],test.impute[,1])
# we use 10 cross validation
set.seed(123)
# we use 10 cross validation
set.seed(123)
regTree.one.hot=rpart(y~., data=train.one.hot, method="anova",control =rpart.control(minsplit =1,minbucket=1,xval=10))
printcp(regTree.one.hot)
# in the following we prune the tree using 1-SE
CVerr=regTree.one.hot$cptable[,"xerror"]
minCP=regTree.one.hot$cptable[which.min(regTree.one.hot$cptable[,"xerror"]),"CP"]
minSE=min(CVerr)+regTree.one.hot$cptable[which.min(CVerr),"xstd"]
w=which.min(CVerr[CVerr>minSE])
wv=which(CVerr>minSE)[w]
CP_SE=regTree.one.hot$cptable[wv,"CP"]
# prune the tress
pruned.tree.one.hot <- prune(regTree.one.hot, cp = CP_SE)
prp(pruned.tree.one.hot)
# on traning data
rmse_reg(regTree.one.hot,train.one.hot[,-1],train.one.hot[,1])
rmse_reg(pruned.tree.one.hot,train.one.hot[,-1],train.one.hot[,1])
# on test data
rmse_reg(regTree.one.hot,test.one.hot[,-1],test.one.hot[,1])
rmse_reg(pruned.tree.one.hot,test.one.hot[,-1],test.one.hot[,1])
# in other script number of trees 80, nodesize=5 test mtry:
bestmtry <- tuneRF(x=train.one.hot[,-1], y=train.one.hot[,1], stepFactor=1.5, improve=1e-10, ntree=80)
print(bestmtry)
# trains
rf.one.hot = randomForest(x = train.one.hot[,-1],y = train.one.hot[,1],ntree=500,mtry=110,nodesize=5)
# on traning data
rmse_reg(rf.one.hot,train.one.hot[,-1],train.one.hot[,1])
# to fix bug of different levels in train and test
test.one.hot <- rbind(train.one.hot[1, ] , test.one.hot)
test.one.hot <- test.one.hot[-1,]
# on test data
rmse_reg(rf.one.hot,test.one.hot[,-1],test.one.hot[,1])
gb.one.hot = blackboost(y ~ .,data=train.one.hot)
train.one.hot
help(encode_onehot)
library(mltools)
install.packages(mltools)
install.packages('mltools')
library(mltools)
one_hot(data)
dummy = dummyVars(" ~ .", data=data)
dummy
data.one.hot=data.frame(predict(dummy, newdata = data))
data.one.hot
data.one.hot
# we use 10 cross validation
set.seed(123)
regTree.one.hot=rpart(y~., data=train.one.hot, method="anova",control =rpart.control(minsplit =1,minbucket=1,xval=10))
printcp(regTree.one.hot)
# in the following we prune the tree using 1-SE
CVerr=regTree.one.hot$cptable[,"xerror"]
minCP=regTree.one.hot$cptable[which.min(regTree.one.hot$cptable[,"xerror"]),"CP"]
minSE=min(CVerr)+regTree.one.hot$cptable[which.min(CVerr),"xstd"]
w=which.min(CVerr[CVerr>minSE])
wv=which(CVerr>minSE)[w]
CP_SE=regTree.one.hot$cptable[wv,"CP"]
# prune the tress
pruned.tree.one.hot <- prune(regTree.one.hot, cp = CP_SE)
prp(pruned.tree.one.hot)
# on traning data
rmse_reg(regTree.one.hot,train.one.hot[,-1],train.one.hot[,1])
rmse_reg(pruned.tree.one.hot,train.one.hot[,-1],train.one.hot[,1])
# on test data
rmse_reg(regTree.one.hot,test.one.hot[,-1],test.one.hot[,1])
rmse_reg(pruned.tree.one.hot,test.one.hot[,-1],test.one.hot[,1])
# in other script number of trees 80, nodesize=5 test mtry:
bestmtry <- tuneRF(x=train.one.hot[,-1], y=train.one.hot[,1], stepFactor=1.5, improve=1e-10, ntree=80)
print(bestmtry)
# trains
rf.one.hot = randomForest(x = train.one.hot[,-1],y = train.one.hot[,1],ntree=500,mtry=110,nodesize=5)
# on traning data
rmse_reg(rf.one.hot,train.one.hot[,-1],train.one.hot[,1])
# to fix bug of different levels in train and test
test.one.hot <- rbind(train.one.hot[1, ] , test.one.hot)
test.one.hot <- test.one.hot[-1,]
# on test data
rmse_reg(rf.one.hot,test.one.hot[,-1],test.one.hot[,1])
gb.one.hot = blackboost(y ~ .,data=train.one.hot)
train.one.hot = data.one.hot[train.test$train,]
test.one.hot = data.one.hot[train.test$test,]
gb.one.hot = blackboost(y ~ .,data=train.one.hot)
# on traning data
rmse_reg(gb.one.hot,train.one.hot[,-1],train.one.hot[,1])
# on test data
rmse_reg(gb.one.hot,test.one.hot[,-1],test.one.hot[,1])
## ELASTIC NET ##
mse1se <- c()
mseMin <- c()
mse1seOut <- c()
x.train.hot = data.one.hot[,-1]
y.train.hot = data.one.hot[,1]
mse1se <- c()
mseMin <- c()
mse1seOut <- c()
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
plot((0:100)/100,mse1se,xlab=expression(alpha),main=expression(paste("MSE w. largest ",lambda," within 1 SE of ",lambda[min])))
x.train.hot = data.one.hot[,-1]
y.train.hot = data.one.hot[,1]
mse1se <- c()
mseMin <- c()
mse1seOut <- c()
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
u
i
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian")
typeof(x.train,hot)
typeof(x.train.hot)
typeof(data.one.hot)
data.one.hot=data.frame(predict(dummy, newdata = data))
typeof(data.one.hot)
x.train.hot
# one hot encode
dummy = dummyVars(" ~ .", data=data.impute)
data.one.hot=data.frame(predict(dummy, newdata = data.impute))
# we use 10 cross validation
set.seed(123)
regTree.one.hot=rpart(y~., data=train.one.hot, method="anova",control =rpart.control(minsplit =1,minbucket=1,xval=10))
printcp(regTree.one.hot)
# in the following we prune the tree using 1-SE
CVerr=regTree.one.hot$cptable[,"xerror"]
minCP=regTree.one.hot$cptable[which.min(regTree.one.hot$cptable[,"xerror"]),"CP"]
minSE=min(CVerr)+regTree.one.hot$cptable[which.min(CVerr),"xstd"]
w=which.min(CVerr[CVerr>minSE])
wv=which(CVerr>minSE)[w]
CP_SE=regTree.one.hot$cptable[wv,"CP"]
# prune the tress
pruned.tree.one.hot <- prune(regTree.one.hot, cp = CP_SE)
prp(pruned.tree.one.hot)
# on traning data
rmse_reg(regTree.one.hot,train.one.hot[,-1],train.one.hot[,1])
rmse_reg(pruned.tree.one.hot,train.one.hot[,-1],train.one.hot[,1])
# on test data
rmse_reg(regTree.one.hot,test.one.hot[,-1],test.one.hot[,1])
rmse_reg(pruned.tree.one.hot,test.one.hot[,-1],test.one.hot[,1])
# in other script number of trees 80, nodesize=5 test mtry:
bestmtry <- tuneRF(x=train.one.hot[,-1], y=train.one.hot[,1], stepFactor=1.5, improve=1e-10, ntree=80)
# one hot encode
dummy = dummyVars(" ~ .", data=data.impute)
data.one.hot=data.frame(predict(dummy, newdata = data.impute))
data.one.hot
train.one.hot = data.one.hot[train.test$train,]
test.one.hot = data.one.hot[train.test$test,]
# we use 10 cross validation
set.seed(123)
regTree.one.hot=rpart(y~., data=train.one.hot, method="anova",control =rpart.control(minsplit =1,minbucket=1,xval=10))
printcp(regTree.one.hot)
# in the following we prune the tree using 1-SE
CVerr=regTree.one.hot$cptable[,"xerror"]
minCP=regTree.one.hot$cptable[which.min(regTree.one.hot$cptable[,"xerror"]),"CP"]
minSE=min(CVerr)+regTree.one.hot$cptable[which.min(CVerr),"xstd"]
w=which.min(CVerr[CVerr>minSE])
wv=which(CVerr>minSE)[w]
CP_SE=regTree.one.hot$cptable[wv,"CP"]
# prune the tress
pruned.tree.one.hot <- prune(regTree.one.hot, cp = CP_SE)
prp(pruned.tree.one.hot)
# on traning data
rmse_reg(regTree.one.hot,train.one.hot[,-1],train.one.hot[,1])
rmse_reg(pruned.tree.one.hot,train.one.hot[,-1],train.one.hot[,1])
# on test data
rmse_reg(regTree.one.hot,test.one.hot[,-1],test.one.hot[,1])
rmse_reg(pruned.tree.one.hot,test.one.hot[,-1],test.one.hot[,1])
# in other script number of trees 80, nodesize=5 test mtry:
bestmtry <- tuneRF(x=train.one.hot[,-1], y=train.one.hot[,1], stepFactor=1.5, improve=1e-10, ntree=80)
print(bestmtry)
# trains
rf.one.hot = randomForest(x = train.one.hot[,-1],y = train.one.hot[,1],ntree=500,mtry=110,nodesize=5)
# on traning data
rmse_reg(rf.one.hot,train.one.hot[,-1],train.one.hot[,1])
# to fix bug of different levels in train and test
test.one.hot <- rbind(train.one.hot[1, ] , test.one.hot)
test.one.hot <- test.one.hot[-1,]
# on test data
rmse_reg(rf.one.hot,test.one.hot[,-1],test.one.hot[,1])
gb.one.hot = blackboost(y ~ .,data=train.one.hot)
x.train.hot = data.one.hot[,-1]
y.train.hot = data.one.hot[,1]
mse1se <- c()
mseMin <- c()
mse1seOut <- c()
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
x.train.hot = as.matrix(data.one.hot[,-1])
y.train.hot = as.matrix(data.one.hot[,1])
mse1se <- c()
mseMin <- c()
mse1seOut <- c()
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
y.train.hot = as.vector(data.one.hot[,1])
mse1se <- c()
mseMin <- c()
mse1seOut <- c()
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test.hot - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test.hot - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
x.train.hot = as.matrix(train.one.hot[,-1])
y.train.hot = as.vector(train.one.hot[,1])
x.train.hot = as.matrix(test.one.hot[,-1])
y.train.hot = as.vector(test.one.hot[,1])
mse1se <- c()
mseMin <- c()
mse1seOut <- c()
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test.hot - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test.hot - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
x.test.hot = as.matrix(test.one.hot[,-1])
y.test.hot = as.vector(test.one.hot[,1])
mse1se <- c()
mseMin <- c()
mse1seOut <- c()
for (i in 0:100) {
assign(paste("fit", i, sep=""), cv.glmnet(x.train.hot, y.train.hot, type.measure="mse",
alpha=i/100,family="gaussian"))
mse1se = append(mse1se, mean((y.test.hot - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x.test.hot))^2))
mseMin = append(mseMin, mean((y.test.hot - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.min, newx=x.test.hot))^2))
#mse1seOut = append(mse1seOut, mean((y_out - predict(eval(parse(text =  paste("fit", i, sep=""))), s=eval(parse(text =  paste("fit", i, sep="")))$lambda.1se, newx=x_out))^2))
}
plot((0:100)/100,mse1se,xlab=expression(alpha),main=expression(paste("MSE w. largest ",lambda," within 1 SE of ",lambda[min])))
plot((0:100)/100,mseMin,xlab=expression(alpha),main=expression(paste("MSE w. ",lambda[min])))
# on traning data
rmse_reg(gb.one.hot,train.one.hot[,-1],train.one.hot[,1])
# on test data
rmse_reg(gb.one.hot,test.one.hot[,-1],test.one.hot[,1])
